---
title: "Performance des modèles"
format: html
echo: false
---

# Introduction

Cette note vise à synthétiser des éléments relatifs à l'évaluation de la qualité du modèle de codification automatique dans la COICOP entraîné à partir des données de caisse de l'IPC. Il s'agit de tirer des conclusions de la première phase de travaux afin d'évaluer la pertinence de la mise en oeuvre d'une stratégie raffinée d'entraînement et d'inférence. 

Pour cela, nous avons adopté l'approche suivante:

* Le modèle est entraîné __exclusivement__ sur les données de caisse selon une approche traditionnelle avec un découpage en échantilllon d'entraînement et de validation. 
* Le jeu d'annotation, dont la nature sera précisée ultérieurement, est utilisé en inférence pour déduire des qualités d'extrapolation sur les données de caisse


# Données utilisées

## Données de caisse

Pour constituer le jeu de données de caisse, nous avons extrait un mois de données de caisse des données de production de l'IPC. Le code ayant permis de constituer celles-ci est sur [gitlab.insee.fr/](https://gitlab.insee.fr/ssplab/extraction-ddc) et les données en question sont stockées sur S3 (`s3://travail/projet-ml-classification-bdf/confidentiel/personnel_sensible/data/raw/sample/ddc.parquet`). 

Ce fichier comporte environ 2.9 millions de produits uniques. Cela fait environ 2.3 millions de données dans l'ensemble d'entraînement contre 600 000 dans l'ensemble de validation. 


```{python}
#| output: false
from src.config.duckdb import create_connection
con = create_connection()
```

## Annotations

```{python}
#| output: false
FILENAME = "annotations_export_2025-06-18_a20fb69ccf494167b816379ca529a12a.parquet"
path = f"projet-budget-famille/data/output-predictions/{FILENAME}"

predictions = con.query(f"""
FROM read_parquet('s3://{path}')
SELECT
    product, code, coicop, method,
    COLUMNS('prediction_\d+'), 
    COLUMNS('confidence_\d+'),
    confidence_0 - confidence_1 AS indice_confiance
""").to_df()
```

Les annotations sont une collection issue de trois sources:

* 


```{python}
import duckdb
from great_tables import GT, md, html
collection_method = duckdb.sql(
    "FROM predictions SELECT method, COUNT(*) AS n GROUP BY method"
).to_df()
```

```{python}
tab = GT(collection_method).fmt_nanoplot(columns="n", plot_type="bar")
print(tab)
```


```{python}
def match_on_dot_level(df, col1, col2, n_points=2):
    """
    Compare deux colonnes de type string dans un DataFrame, en gardant
    uniquement les `n_points` premiers segments séparés par un point (.),
    puis calcule la proportion de lignes identiques.

    Args:
        df (pd.DataFrame): Le DataFrame contenant les colonnes à comparer.
        col1 (str): Nom de la première colonne.
        col2 (str): Nom de la seconde colonne.
        n_points (int): Nombre de segments à garder avant comparaison.

    Returns:
        float: Proportion d’égalité ligne à ligne après découpe.
    """
    left = df[col1].str.split('.', n=n_points).str[:n_points].str.join('.')
    right = df[col2].str.split('.', n=n_points).str[:n_points].str.join('.')
    return float(
        (left == right).mean()
    )

```



```{python}
perf_whole_annotations = {
    f"whole_annotations_level{n}": match_on_dot_level(predictions, "prediction_0", "code", n_points=n)
    for n in range(1, 6)
}
```

```{python}
perf_whole_annotations = {
    f"whole_annotations_level{n}": match_on_dot_level(predictions, "prediction_0", "code", n_points=n)
    for n in range(1, 6)
}
```


```{python}
perf_annotations_by_method = {}

for method, subset in predictions.groupby("method"):
    for n in range(1, 6):
        key = f"{method}_annotations_level{n}"
        perf_annotations_by_method[key] = match_on_dot_level(subset, "prediction_0", "code", n_points=n)

```


```{python}
perf_ddc = {
    "ddc_validation_level1": 0.9597685621344899,
    "ddc_validation_level2": 0.9497502533018113,
    "ddc_validation_level3": 0.9347370105053593
}

```


```{python}
import pandas as pd
import re

all_dicts = [perf_ddc, perf_whole_annotations, perf_annotations_by_method]

# Liste pour stocker les lignes
rows = []

# Extraire les infos de chaque dict
for d in all_dicts:
    for key, value in d.items():
        match = re.match(r"(.*)_level(\d+)", key)
        if match:
            source = match.group(1)
            level = f"level{match.group(2)}"
            rows.append({
                "source": source,
                "niveau": level,
                "perf": value
            })

# Créer le DataFrame final
df = pd.DataFrame(rows)
df = df.sort_values(by=["niveau", "source"]).reset_index(drop=True)

df
```



```{python}
recoding = {
    "ddc_validation": "Données de validation (DDC)",
    "whole_annotations": "Données annotées BdF (ensemble)",
    "Produits issus de l'application (carnets)_annotations": "Données annotées BdF (carnets)",
    "Produits issus de l'application (tickets)_annotations": "Données annotées BdF (tickets de caisse)",
    "manual_annotations": "Annotations manuelles (Copain ou LibreOffice)"
}
df["source_label"] = df["source"].map(recoding)
df["complete"] = df["source"].isin(["ddc_validation", "whole_annotations"])
df["niveau"] = df["niveau"].str.replace("level", "").astype(int)
df=df.loc[df['niveau']<4]
```



```{python}
from plotnine import *

# Création de la figure
p = (
    ggplot(df, aes(x="niveau", y="perf", color="source_label", linetype="complete"))
    + geom_line()
    + geom_point() 
    + scale_linetype_manual(values={False: "dashed", True: "solid"})
    + labs(
        title="Performance par niveau et source",
        x="Niveau",
        y="Performance",
        color="Source",
        linetype="Complétude"
    )
    + theme(legend_position="bottom")

)

p
```